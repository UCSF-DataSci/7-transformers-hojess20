{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a3f084",
   "metadata": {},
   "source": [
    "# Part 2: Basic LLM Chat Tool\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, you'll create a simple command-line chat tool that interacts with a Large Language Model (LLM) through the Hugging Face API. This tool will allow you to have conversations with an LLM about healthcare topics.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Connect to the Hugging Face API\n",
    "- Create a basic interactive chat loop\n",
    "- Handle simple error cases\n",
    "- Test with healthcare questions\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69749a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Additional packages for LLM API interaction\n",
    "%pip install requests\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('utils', exist_ok=True)\n",
    "os.makedirs('results/part_2', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c33bc6",
   "metadata": {},
   "source": [
    "## 1. Connecting to the Hugging Face API\n",
    "\n",
    "The Hugging Face Inference API provides access to many language models. We'll use models that are available on the free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f6dbb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a swollen rectum\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "# Example of a simple API request to Hugging Face\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.getenv('HF_API_KEY')}\"}  # Optional for some models\n",
    "\n",
    "def query(payload):\n",
    "    \"\"\"\n",
    "    Send a query to the Hugging Face API\n",
    "    \n",
    "    Args:\n",
    "        payload: Dictionary containing the query parameters\n",
    "        \n",
    "    Returns:\n",
    "        The API response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the API request\n",
    "    # Use requests.post to send the query to the API_URL\n",
    "    # Return the response\n",
    "    input_text = payload.get(\"inputs\", \"\")\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "# Test the query function\n",
    "test_payload = {\"inputs\": \"What are the symptoms of diabetes?\"}\n",
    "response = query(test_payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8b3f1",
   "metadata": {},
   "source": [
    "## 2. Creating Simple Chat Scripts\n",
    "\n",
    "Your task is to create two simple scripts that interact with the Hugging Face API:\n",
    "\n",
    "1. A basic one-off chat script (`utils/one_off_chat.py`)\n",
    "2. A contextual conversation script (`utils/conversation.py`)\n",
    "\n",
    "### One-Off Chat Script\n",
    "\n",
    "Create a script that handles independent interactions (each prompt/response is separate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f2e08e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Simple LLM Chat! Type 'exit' to quit.\n",
      "Model: Request failed: 401 Client Error: Unauthorized for url: https://api-inference.huggingface.co/models/google/flan-t5-base\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# utils/one_off_chat.py\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_response(prompt, model_name=\"google/flan-t5-base\", api_key=None):\n",
    "    \"\"\"\n",
    "    Get a response from the model\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication (optional for some models)\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the get_response function\n",
    "    # Set up the API URL and headers\n",
    "    # Create a payload with the prompt\n",
    "    # Send the payload to the API\n",
    "    # Extract and return the generated text from the response\n",
    "    # Handle any errors that might occur\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    headers = {}\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    payload = {\"inputs\": prompt}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        if isinstance(result, list) and \"generated_text\" in result[0]:\n",
    "            return result[0][\"generated_text\"]\n",
    "        else:\n",
    "            return str(result)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {str(e)}\"\n",
    "\n",
    "def run_chat(model_name, api_key):\n",
    "    \"\"\"Run an interactive chat session\"\"\"\n",
    "    print(\"Welcome to the Simple LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        # TODO: Get response from the model\n",
    "        response = get_response(user_input, model_name, api_key)\n",
    "        print(f\"Model: {response}\")\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    parser.add_argument('--model', type=str, default=\"google/flan-t5-base\", help=\"Model name to use\")\n",
    "    parser.add_argument('--api_key', type=str, default=os.getenv(\"HF_API_KEY\"), help=\"Hugging Face API key\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    run_chat(model_name=args.model, api_key=args.api_key)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092d4a8",
   "metadata": {},
   "source": [
    "### Contextual Conversation Script\n",
    "\n",
    "Create a script that maintains conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c67cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Hugging Face API key using --api_key or the HF_API_KEY environment variable.\n"
     ]
    }
   ],
   "source": [
    "# utils/conversation.py\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_response(prompt, history=None, model_name=\"google/flan-t5-base\", api_key=None, history_length=3):\n",
    "    \"\"\"\n",
    "    Get a response from the model using conversation history\n",
    "    \n",
    "    Args:\n",
    "        prompt: The current user prompt\n",
    "        history: List of previous (prompt, response) tuples\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        history_length: Number of previous exchanges to include in context\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the contextual response function\n",
    "    # Initialize history if None\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    # TODO: Format a prompt that includes previous exchanges\n",
    "    # Get a response from the API\n",
    "    # Return the response\n",
    "    # Use the last few exchanges to build context\n",
    "    recent_history = history[-history_length:]\n",
    "    context = \"\"\n",
    "    for i, (prev_prompt, prev_response) in enumerate(recent_history):\n",
    "        context += f\"User: {prev_prompt}\\nAssistant: {prev_response}\\n\"\n",
    "    \n",
    "    # Append the current prompt\n",
    "    context += f\"User: {prompt}\\nAssistant:\"\n",
    "\n",
    "    # Prepare API request\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    headers = {}\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    payload = {\"inputs\": context}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "\n",
    "        if isinstance(result, list) and \"generated_text\" in result[0]:\n",
    "            full_text = result[0][\"generated_text\"]\n",
    "            # Extract just the latest reply (after \"Assistant:\")\n",
    "            assistant_reply = full_text.split(\"Assistant:\")[-1].strip()\n",
    "            return assistant_reply\n",
    "        else:\n",
    "            return str(result)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {str(e)}\"\n",
    "    \n",
    "\n",
    "def run_chat(model_name, api_key, history_length=3):\n",
    "    \"\"\"Run an interactive chat session with context\"\"\"\n",
    "    print(\"Welcome to the Contextual LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        # TODO: Get response using conversation history\n",
    "        # Update history\n",
    "        # Print the response\n",
    "        # Get response from the model\n",
    "        response = get_response(user_input, history, model_name, api_key, history_length)\n",
    "        \n",
    "        # Update history with this exchange\n",
    "        history.append((user_input, response))\n",
    "\n",
    "        # Print the model's reply\n",
    "        print(f\"Assistant: {response}\")\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM using conversation history\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    parser.add_argument('--model', type=str, default=\"google/flan-t5-base\", help=\"Model name to use\")\n",
    "    parser.add_argument('--api_key', type=str, default=os.getenv(\"HF_API_KEY\"), help=\"Hugging Face API key\")\n",
    "    parser.add_argument('--history_length', type=int, default=3, help=\"Number of past exchanges to include in context\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()  \n",
    "\n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    if not args.api_key:\n",
    "        print(\"Please provide your Hugging Face API key using --api_key or the HF_API_KEY environment variable.\")\n",
    "        return\n",
    "\n",
    "    run_chat(model_name=args.model, api_key=args.api_key, history_length=args.history_length)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54142b18",
   "metadata": {},
   "source": [
    "## 3. Testing and Evaluation\n",
    "\n",
    "Create a script to test your chat implementations with specific healthcare questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b023ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/test_chat.py\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our chat modules - since we're in the same directory\n",
    "from one_off_chat import get_response as get_one_off_response\n",
    "# Optionally import the conversation module if testing that too\n",
    "# from conversation import get_response as get_contextual_response\n",
    "\n",
    "def test_chat(questions, model_name=\"google/flan-t5-base\", api_key=None):\n",
    "    \"\"\"\n",
    "    Test the chat function with a list of questions\n",
    "    \n",
    "    Args:\n",
    "        questions: A list of questions to test\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping questions to responses\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"Testing question: {question}\")\n",
    "        # Get response using the one-off chat function\n",
    "        response = get_one_off_response(question, model_name, api_key)\n",
    "        results[question] = response\n",
    "        \n",
    "    return results\n",
    "\n",
    "# List of healthcare questions to test\n",
    "test_questions = [\n",
    "    \"What are the symptoms of gout?\",\n",
    "    \"How is gout diagnosed?\",\n",
    "    \"What treatments are available for gout?\",\n",
    "    \"What lifestyle changes can help manage gout?\",\n",
    "    \"What foods should be avoided with gout?\"\n",
    "]\n",
    "\n",
    "def save_results(results, output_file=\"results/part_2/example.txt\"):\n",
    "    \"\"\"\n",
    "    Save the test results to a file\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary mapping questions to responses\n",
    "        output_file: Path to the output file\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"# LLM Chat Tool Test Results\\n\\n\")\n",
    "        \n",
    "        # Write usage examples\n",
    "        f.write(\"## Usage Examples\\n\\n\")\n",
    "        f.write(\"```bash\\n\")\n",
    "        f.write(\"# Run the one-off chat\\n\")\n",
    "        f.write(\"python utils/one_off_chat.py\\n\\n\")\n",
    "        f.write(\"# Run the contextual chat\\n\")\n",
    "        f.write(\"python utils/conversation.py\\n\")\n",
    "        f.write(\"```\\n\\n\")\n",
    "        \n",
    "        # Write test results\n",
    "        f.write(\"## Test Results\\n\\n\")\n",
    "        f.write(\"```csv\\n\")\n",
    "        f.write(\"question,response\\n\")\n",
    "        \n",
    "        for question, response in results.items():\n",
    "            # Format the question and response for CSV\n",
    "            q = question.replace(',', '').replace('\\n', ' ')\n",
    "            r = response.replace(',', '').replace('\\n', ' ')\n",
    "            f.write(f\"{q},{r}\\n\")\n",
    "            \n",
    "        f.write(\"```\\n\")\n",
    "\n",
    "# Run the test and save results\n",
    "if __name__ == \"__main__\":\n",
    "    results = test_chat(test_questions)\n",
    "    save_results(results)\n",
    "    print(\"Test results saved to results/part_2/example.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982ac2d",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **API Connection**:\n",
    "   - [ ] Successfully connect to the Hugging Face API\n",
    "   - [ ] Send a query and receive a response\n",
    "   - [ ] Handle API errors gracefully\n",
    "\n",
    "2. **Chat Function Implementation**:\n",
    "   - [ ] Implement the get_response function\n",
    "   - [ ] Create the run_chat function for interactive sessions\n",
    "   - [ ] Handle errors and edge cases\n",
    "\n",
    "3. **Command Line Interface**:\n",
    "   - [ ] Create a parser with appropriate arguments\n",
    "   - [ ] Implement the main function\n",
    "   - [ ] Test the CLI functionality\n",
    "\n",
    "4. **Testing and Evaluation**:\n",
    "   - [ ] Test the functions with healthcare questions\n",
    "   - [ ] Save the results in a structured format\n",
    "   - [ ] Analyze the quality of responses\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "1. **API Access Issues**:\n",
    "   - Problem: Rate limiting\n",
    "   - Solution: Implement exponential backoff and retry logic\n",
    "   - Problem: Authentication errors\n",
    "   - Solution: Verify API key and environment variables\n",
    "\n",
    "2. **Response Parsing Issues**:\n",
    "   - Problem: Unexpected response format\n",
    "   - Solution: Add error handling for different response structures\n",
    "   - Problem: Empty or error responses\n",
    "   - Solution: Provide meaningful fallback responses\n",
    "\n",
    "3. **CLI Issues**:\n",
    "   - Problem: Arguments not parsed correctly\n",
    "   - Solution: Test with different argument combinations\n",
    "   - Problem: Script not executable\n",
    "   - Solution: Check file permissions\n",
    "\n",
    "## What to Submit\n",
    "\n",
    "1. Your implementation of the chat scripts:\n",
    "   - Basic requirement: `utils/one_off_chat.py` for single prompt/response chat\n",
    "   - Stretch goal (optional): `utils/conversation.py` for contextual chat\n",
    "   - Testing script: `utils/test_chat.py` to evaluate your implementation\n",
    "\n",
    "2. Test results in `results/part_2/example.txt` with the following format:\n",
    "   - Usage examples section showing how to run your scripts\n",
    "   - Test results section with CSV-formatted question/response pairs\n",
    "   - If you implemented the stretch goal, include examples of contextual exchanges\n",
    "\n",
    "The auto-grader should check:\n",
    "1. That your chat scripts can be executed\n",
    "2. That they correctly handle the test questions\n",
    "3. That your results file contains the required sections"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (hw7)",
   "language": "python",
   "name": "hw7-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
